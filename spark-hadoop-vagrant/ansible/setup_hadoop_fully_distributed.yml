---
- hosts: local
  gather_facts: False
  vars:
    var_hadoop_version: "3.2.0"
  tasks:
    - name: Get Hadoop
      get_url:
        url: "https://www-us.apache.org/dist/hadoop/common/hadoop-{{ var_hadoop_version }}/hadoop-{{ var_hadoop_version }}.tar.gz"
        dest: resources/
        mode: 0776

- hosts: cluster
  gather_facts: False
  become: True
  vars:
    var_hadoop_version: "3.2.0"
    var_apps_home: "/usr/local"
    var_java_home: "/usr/lib/jvm/java-1.8.0-openjdk"
    var_hadoop_home: "{{ var_apps_home }}/hadoop"
    var_create_user: True
    var_user_name: "hduser"
    # Generate passworkd hash with: ansible all -m debug -a "msg={{ 'hduser' | password_hash('sha512', 'hduser')}}"
    var_user_psw: $6$hduser$sn14ciKozJ/aOrye3jpD1uQoF.OXh/2stFyaVvx/GkDH76tEg3h013W6.nKyCwVggG3/M3NXcJpQkODUlnm7B.
  tasks:
    - name: Ensure Java 1.8 Devel and tools are installed
      yum: 
        name: ['iptables-services', 'nano', 'net-tools', 'telnet', 'java-1.8.0-openjdk-devel'] 
        state: 'present' 
        update_cache: 'yes'

    # Bring down the firewall so don't interfere with communication between nodes
    - name: Bring down the firewall
      service: name=iptables state=stopped enabled=no

    - name: Ensure hadoop group exists
      group: name=hadoop state=present

    - name: Ensure {{ var_user_name }} exists and is added to the hadoop group
      user: 
        name: "{{ var_user_name }}"
        groups: hadoop
        state: present
        password: "{{ var_user_psw }}"
        generate_ssh_key: yes
        ssh_key_bits: 2048
        ssh_key_file: .ssh/id_rsa
      when: {{ var_create_user }}

    # Ensure non strict host key checking as might interfere with automated startup
    - name: Ensure no Strict Host Key Checking
      copy:
        src: resources/configs/ssh_config
        dest: "/home/{{ var_user_name }}/.ssh/config"
        owner: "{{ var_user_name }}"
        group: "{{ var_user_name }}"
      when: {{ var_create_user }}

    - name: Ensure {{ var_user_name }} has sudo with passwordless privileges
      lineinfile: 
        path: "/etc/sudoers.d/{{ var_user_name }}"
        create: yes
        state: present
        line: "%{{ var_user_name }} ALL=(ALL) NOPASSWD: ALL"
      when: {{ var_create_user }}

    - name: Get public key from hosts
      fetch: 
        src: "/home/{{ var_user_name }}/.ssh/id_rsa.pub"
        dest: "/keys"

    - name: Distribute public keys are distributed among cluster members
      authorized_key:
        user: "{{ var_user_name }}"
        state: present
        key: "{{ item }}"
      #with_items: "{{ '/keys/*/home/hduser/.ssh/id_rsa.pub' | fileglob }}"
      with_file:
        - "/keys/node1/home/{{ var_user_name }}/.ssh/id_rsa.pub"
        - "/keys/node2/home/{{ var_user_name }}/.ssh/id_rsa.pub"
        - "/keys/node3/home/{{ var_user_name }}/.ssh/id_rsa.pub"
        - "/keys/node4/home/{{ var_user_name }}/.ssh/id_rsa.pub"
        - "/keys/node5/home/{{ var_user_name }}/.ssh/id_rsa.pub"
    
    - name: Ensure JAVA_HOME environment variable is setup
      lineinfile:
        path: "/home/{{ var_user_name }}/.bashrc"
        create: yes
        state: present
        line: "export JAVA_HOME={{ var_java_home }}"

    - name: Inflate Hadoop on {{ var_apps_home }}
      unarchive:
        src: "resources/hadoop-{{ var_hadoop_version }}.tar.gz"
        dest: "{{ var_apps_home }}"
        owner: "{{ var_user_name }}"
        group: "{{ var_user_name }}"

    - name: Ensure symlink to hadoop
      file: 
        src: "/usr/local/hadoop-{{ var_hadoop_version }}"
        dest: "{{ var_hadoop_home }}"
        state: link
        owner: "{{ var_user_name }}"
        group: "{{ var_user_name }}"

    - name: Copy Hadoop configuration files
      copy:
        src: resources/configs/fully-distributed/{{ item }}
        dest: "{{ var_hadoop_home }}/etc/hadoop"
        owner: "{{ var_user_name }}"
        group: "{{ var_user_name }}"
      with_items:
        - core-site.xml
        - hdfs-site.xml
        - mapred-site.xml
        - yarn-site.xml
        - workers

    - name: Ensure HADOOP_HOME environment variable is set
      lineinfile:
        path: "/home/{{ var_user_name }}/.bashrc"
        create: yes
        state: present
        line: "export HADOOP_HOME={{ var_hadoop_home }}"

    - name: Ensure JAVA_HOME and HADOOP_HOME is added to PATH for {{ var_user_name }}
      lineinfile:
        path: "/home/{{ var_user_name }}/.bashrc"
        create: yes
        state: present
        line: "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin"

    # Clear automatically the logs when the machine is started up
    - name: Ensure tmp clean after init scripts
      lineinfile:
        path: /etc/rc.d/rc.local
        create: yes
        state: present
        line: "/bin/rm -rf /tmp/*"

    # Configuration files on hadoop
    - name: Ensure JAVA_HOME variable on *-env.sh is set correctly
      lineinfile:
        path: "{{ var_hadoop_home }}/etc/hadoop/hadoop-env.sh"
        regexp: '^[#\s]{0,2}export JAVA_HOME=.+'
        line: 'export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk'
      with_items:
        - hadoop-env.sh
        - mapred-env.sh
        - yarn-env.sh

    - name: Ensure HEAP_SIZE variable on hadoop-env.sh is set to 200
      lineinfile:
        path: "{{ var_hadoop_home }}/etc/hadoop/hadoop-env.sh"
        regexp: '^#export HADOOP_HEAPSIZE='
        line: 'export HADOOP_HEAPSIZE=200'


- hosts: namenode
  gather_facts: False
  become: True
  tasks:
    # This is important as avoids running the namenode using 127.0.0.1 which will prevent slaves nodes to connect
    - name: Ensure /etc/hosts maps for node1 are not pointing to 127.0.0.1 only
      lineinfile:
        path: /etc/hosts
        regexp: '^127.0.0.1	node1	node1'
        line: '#127.0.0.1	node1	node1'
    # Final preparations to bring up the pseudo distributed mode
    - name: Format the namenode
      shell: bin/hdfs namenode -format playground-cluster
      args:
        chdir: "{{ var_hadoop_home }}"
        creates: "/home/{{ var_user_name }}/namenode/current/fsimage_0000000000000000000"
      become: yes
      become_user: "{{ var_user_name }}"

    # In case of failing to connect to namenode because it is not started and need to be
    # Restarted again, command sbin/hadoop-daemon.sh start namenode needs to be executed
    - name: Ensure Namenode is started
      shell: sbin/start-dfs.sh
      args:
        chdir: "{{ var_hadoop_home }}"
        creates: "/tmp/hadoop-{{ var_user_name }}/hadoop-{{ var_user_name }}-namenode.pid"
      become: yes
      become_user: "{{ var_user_name }}"

- hosts: yarn
  gather_facts: False
  become: True
  tasks:
    - name: Ensure YARN daemons are started
      shell: sbin/start-yarn.sh
      args:
        chdir: "{{ var_hadoop_home }}"
        creates: "/tmp/hadoop-{{ var_user_name }}/yarn-{{ var_user_name }}-nodemanager.pid"
      become: yes
      become_user: "{{ var_user_name }}"

- hosts: mrjobhistory
  gather_facts: False
  become: True
  tasks:
    - name: Ensure Job History Server daemons are started
      shell: sbin/mr-jobhistory-daemon.sh --config /usr/local/hadoop/etc/hadoop/ start historyserver
      args:
        chdir: "{{ var_hadoop_home }}"
        creates: "/tmp/hadoop-{{ var_user_name }}/yarn-{{ var_user_name }}-nodemanager.pid"
      become: yes
      become_user: "{{ var_user_name }}"

